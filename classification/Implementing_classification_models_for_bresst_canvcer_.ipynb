{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üß† Introduction (Beginner Summary)\n",
        "\n",
        "In this activity, you‚Äôll learn how to build and test different machine learning models that can classify data into categories ‚Äî just like how AI can tell if an email is spam or not.\n",
        "\n",
        "You‚Äôll use Python to create models such as logistic regression, decision trees, and support vector machines (SVMs). You‚Äôll also learn how to prepare your data, train the models, and compare how well they perform.\n",
        "\n",
        "ü™ú What You‚Äôll Do Step-by-Step\n",
        "\n",
        "Set up your environment ‚Äì Create a new Jupyter Notebook and use the right Python version.\n",
        "\n",
        "Load and explore the dataset ‚Äì Open your data and take a look at what it contains.\n",
        "\n",
        "Preprocess the data ‚Äì Clean and prepare the data so the models can understand it.\n",
        "\n",
        "Build a Logistic Regression model ‚Äì A simple model that predicts between categories.\n",
        "\n",
        "Build a Decision Tree model ‚Äì A visual, step-by-step model that splits data to make decisions."
      ],
      "metadata": {
        "id": "kbB5aar_2w8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Set up the environment\n",
        "Instructions\n",
        "\n",
        "First, ensure you have the necessary libraries installed. We‚Äôll be using Scikit-Learn for machine learning models, pandas for data manipulation, and matplotlib or seaborn for visualization.\n",
        "\n",
        "Install the required libraries using the following commands:"
      ],
      "metadata": {
        "id": "QcrdTLwX20m-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avxIeGXso4hz",
        "outputId": "db23f18d-f994-4cee-9a77-73c8dc6ba51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "These libraries will provide the tools to load, manipulate, and visualize the dataset, as well as implement and evaluate classification models."
      ],
      "metadata": {
        "id": "-McN8BkN3EvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Step 2: Load and Explore the Dataset (Summary)\n",
        "\n",
        "In this step, you‚Äôll load your dataset and take a closer look at what‚Äôs inside before building your model.\n",
        "\n",
        "You‚Äôll:\n",
        "\n",
        "Download the dataset that contains both the inputs (features) and the outputs (labels) for your classification task.\n",
        "\n",
        "Load it into a pandas DataFrame so you can easily view and analyze it.\n",
        "\n",
        "Explore the data by checking for missing values, data types, and using commands like .head() to see the first few rows."
      ],
      "metadata": {
        "id": "EMIX5rmE5GjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Breast Cancer dataset and convert to DataFrame\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Explore the dataset\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJAFx0px5J9f",
        "outputId": "4edc74e2-4797-4936-8764-566ab66357ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   mean radius              569 non-null    float64\n",
            " 1   mean texture             569 non-null    float64\n",
            " 2   mean perimeter           569 non-null    float64\n",
            " 3   mean area                569 non-null    float64\n",
            " 4   mean smoothness          569 non-null    float64\n",
            " 5   mean compactness         569 non-null    float64\n",
            " 6   mean concavity           569 non-null    float64\n",
            " 7   mean concave points      569 non-null    float64\n",
            " 8   mean symmetry            569 non-null    float64\n",
            " 9   mean fractal dimension   569 non-null    float64\n",
            " 10  radius error             569 non-null    float64\n",
            " 11  texture error            569 non-null    float64\n",
            " 12  perimeter error          569 non-null    float64\n",
            " 13  area error               569 non-null    float64\n",
            " 14  smoothness error         569 non-null    float64\n",
            " 15  compactness error        569 non-null    float64\n",
            " 16  concavity error          569 non-null    float64\n",
            " 17  concave points error     569 non-null    float64\n",
            " 18  symmetry error           569 non-null    float64\n",
            " 19  fractal dimension error  569 non-null    float64\n",
            " 20  worst radius             569 non-null    float64\n",
            " 21  worst texture            569 non-null    float64\n",
            " 22  worst perimeter          569 non-null    float64\n",
            " 23  worst area               569 non-null    float64\n",
            " 24  worst smoothness         569 non-null    float64\n",
            " 25  worst compactness        569 non-null    float64\n",
            " 26  worst concavity          569 non-null    float64\n",
            " 27  worst concave points     569 non-null    float64\n",
            " 28  worst symmetry           569 non-null    float64\n",
            " 29  worst fractal dimension  569 non-null    float64\n",
            " 30  target                   569 non-null    int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 137.9 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Understanding the structure of your dataset is crucial for selecting the right preprocessing steps and models."
      ],
      "metadata": {
        "id": "YUv92Dog5WHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Step 3: Preprocess the Data (Summary)\n",
        "\n",
        "Before training or testing your model, you need to clean and prepare your dataset so it‚Äôs ready for machine learning.\n",
        "\n",
        "You‚Äôll:\n",
        "\n",
        "Decide how to split your data\n",
        "\n",
        "If using a pretrained model, you usually just split the data into training and testing sets (no need for a separate validation set).\n",
        "\n",
        "This is especially useful if your dataset is small or you‚Äôre only evaluating model performance.\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "Fill them using the mean or median of the column, or\n",
        "\n",
        "Remove rows/columns with too many missing entries.\n",
        "\n",
        "Encode categorical data\n",
        "\n",
        "Convert text or category labels into numbers using LabelEncoder or pd.get_dummies() (one-hot encoding).\n",
        "\n",
        "Split the dataset\n",
        "\n",
        "Use train_test_split from Scikit-Learn to divide data into 80% training and 20% testing.\n",
        "\n",
        "Set a random seed so you get the same split every time.\n",
        "\n",
        "Verify the split\n",
        "\n",
        "Check the shapes of your training and testing data to ensure everything worked correctly."
      ],
      "metadata": {
        "id": "BFR1eXVqEiNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Handle missing data (example: filling missing values with the median)\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7qiCixeKEjzs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Preprocessing ensures that your data is clean and ready for ML models to use. Splitting the dataset into training and test sets allows us to evaluate the model‚Äôs performance on unseen data."
      ],
      "metadata": {
        "id": "bIr5SYp3FRrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Implement a logistic regression model\n",
        "Instructions\n",
        "\n",
        "Train a logistic regression model on the training data, and evaluate its performance on the test data.\n",
        "Steps\n",
        "\n",
        "    Import LogisticRegression from Scikit-Learn.\n",
        "\n",
        "    Train the model using fit().\n",
        "\n",
        "    Predict the labels for the test data, and calculate accuracy."
      ],
      "metadata": {
        "id": "zjiLeDXZFTqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOH-BrfwFjOV",
        "outputId": "65829f9f-be5e-41f5-a368-6955901891e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 95.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Logistic regression is a simple yet effective model for binary classification tasks. Accuracy is one of the metrics used to evaluate how well the model is performing."
      ],
      "metadata": {
        "id": "ticY2w16FyyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Implement a decision tree model\n",
        "\n",
        "Decision trees split the data based on feature values and make decisions at each node.\n",
        "Instructions\n",
        "\n",
        "Train a decision tree model, and evaluate its performance on the test set.\n",
        "Steps\n",
        "\n",
        "    Import DecisionTreeClassifier from Scikit-Learn.\n",
        "\n",
        "    Train the model on the training data.\n",
        "\n",
        "    Make predictions and evaluate the accuracy."
      ],
      "metadata": {
        "id": "bf6gJeu-F8a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train decision tree model\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zwNPgiF_pP",
        "outputId": "c201d659-a641-4f91-94fd-370c78e12f50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 92.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Decision trees are highly interpretable models that make decisions by splitting the data based on the most informative features. However, they can be prone to overfitting if not tuned properly."
      ],
      "metadata": {
        "id": "22d6sgOBGEF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Implement a support vector machine model\n",
        "\n",
        "An SVM model is great for high-dimensional spaces. SVMs find a hyperplane that separates the data points into different classes with maximum margin.\n",
        "Instructions\n",
        "\n",
        "Train a support vector machine (SVM) model, and evaluate its performance on the test set.\n",
        "Steps\n",
        "\n",
        "    Import support vector classifier (SVC) from Scikit-Learn.\n",
        "\n",
        "    Train the model on the training data.\n",
        "\n",
        "    Make predictions and evaluate the accuracy."
      ],
      "metadata": {
        "id": "a5FZWM2BGV8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train SVM model\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8baG-PQjGhBx",
        "outputId": "58104432-4946-40a2-f917-b23894d61bd8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 94.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "SVMs are powerful models, particularly in high-dimensional spaces. They work by finding a hyperplane that separates data points into different classes with the maximum margin."
      ],
      "metadata": {
        "id": "62_KvbnJGqO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Evaluate and compare model performance\n",
        "Instructions\n",
        "\n",
        "Compare the performance of the different models using accuracy, precision, recall, and the F1 score.\n",
        "Steps\n",
        "\n",
        "    Import additional evaluation metrics, including precision_score, recall_score, and f1_score.\n",
        "\n",
        "    Calculate these metrics for each model, and print the results for comparison."
      ],
      "metadata": {
        "id": "CRW8oKhMG4Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Logistic Regression - Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfWCZC2VG9my",
        "outputId": "ae8f8b18-0a17-4dbd-b842-203d97c17324"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Precision: 0.96, Recall: 0.96, F1 Score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "Accuracy is not always the best metric for evaluating classification models, especially with imbalanced datasets. Precision, recall, and the F1 score provide a more complete picture of model performance."
      ],
      "metadata": {
        "id": "ZbzR6JN6HVQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "In this activity, you successfully implemented several classification models using Python, including logistic regression, decision trees, and SVMs. By training and evaluating these models on a dataset, you gained experience in using common metrics to compare their performance. Understanding how different models work and how to evaluate them is crucial for building reliable machine learning systems."
      ],
      "metadata": {
        "id": "RDqu9qyrGgWI"
      }
    }
  ]
}